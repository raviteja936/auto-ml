{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Define the model.\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def build_model(mode, inputs, params):\n",
    "    \"\"\"Compute logits of the model (output distribution)\n",
    "    Args:\n",
    "        mode: (string) 'train', 'eval', etc.\n",
    "        inputs: (dict) contains the inputs of the graph (features, labels...)\n",
    "                this can be `tf.placeholder` or outputs of `tf.data`\n",
    "        params: (Params) contains hyperparameters of the model (ex: `params.learning_rate`)\n",
    "    Returns:\n",
    "        output: (tf.Tensor) output of the model\n",
    "    \"\"\"\n",
    "    sentence = inputs['sentence']\n",
    "\n",
    "    if params.model_version == 'lstm':\n",
    "        # Get word embeddings for each token in the sentence\n",
    "        embeddings = tf.get_variable(name=\"embeddings\", dtype=tf.float32,\n",
    "                shape=[params.vocab_size, params.embedding_size])\n",
    "        sentence = tf.nn.embedding_lookup(embeddings, sentence)\n",
    "\n",
    "        # Apply LSTM over the embeddings\n",
    "        lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(params.lstm_num_units)\n",
    "        output, _  = tf.nn.dynamic_rnn(lstm_cell, sentence, dtype=tf.float32)\n",
    "\n",
    "        # Compute logits from the output of the LSTM\n",
    "        logits = tf.layers.dense(output, params.number_of_tags)\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError(\"Unknown model version: {}\".format(params.model_version))\n",
    "\n",
    "    return logits\n",
    "\n",
    "\n",
    "def model_fn(mode, inputs, params, reuse=False):\n",
    "    \"\"\"Model function defining the graph operations.\n",
    "    Args:\n",
    "        mode: (string) 'train', 'eval', etc.\n",
    "        inputs: (dict) contains the inputs of the graph (features, labels...)\n",
    "                this can be `tf.placeholder` or outputs of `tf.data`\n",
    "        params: (Params) contains hyperparameters of the model (ex: `params.learning_rate`)\n",
    "        reuse: (bool) whether to reuse the weights\n",
    "    Returns:\n",
    "        model_spec: (dict) contains the graph operations or nodes needed for training / evaluation\n",
    "    \"\"\"\n",
    "    is_training = (mode == 'train')\n",
    "    uniq_ids = inputs['uniq_id']\n",
    "    features = inputs['features']\n",
    "    target = inputs['target']\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # MODEL: define the layers of the model\n",
    "    with tf.variable_scope('model', reuse=reuse):\n",
    "        # Compute the output distribution of the model and the predictions\n",
    "        logits = build_model(mode, inputs, params)\n",
    "        predictions = tf.argmax(logits, -1)\n",
    "\n",
    "    # Define loss and accuracy (we need to apply a mask to account for padding)\n",
    "    losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=target)\n",
    "    mask = tf.sequence_mask(sentence_lengths)\n",
    "    losses = tf.boolean_mask(losses, mask)\n",
    "    loss = tf.reduce_mean(losses)\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(target, predictions), tf.float32))\n",
    "\n",
    "    # Define training step that minimizes the loss with the Adam optimizer\n",
    "    if is_training:\n",
    "        optimizer = tf.train.AdamOptimizer(params.learning_rate)\n",
    "        global_step = tf.train.get_or_create_global_step()\n",
    "        train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # METRICS AND SUMMARIES\n",
    "    # Metrics for evaluation using tf.metrics (average over whole dataset)\n",
    "    with tf.variable_scope(\"metrics\"):\n",
    "        metrics = {\n",
    "            'accuracy': tf.metrics.accuracy(labels=target, predictions=predictions),\n",
    "            'loss': tf.metrics.mean(loss)\n",
    "        }\n",
    "\n",
    "    # Group the update ops for the tf.metrics\n",
    "    update_metrics_op = tf.group(*[op for _, op in metrics.values()])\n",
    "\n",
    "    # Get the op to reset the local variables used in tf.metrics\n",
    "    metric_variables = tf.get_collection(tf.GraphKeys.LOCAL_VARIABLES, scope=\"metrics\")\n",
    "    metrics_init_op = tf.variables_initializer(metric_variables)\n",
    "\n",
    "    # Summaries for training\n",
    "    tf.summary.scalar('loss', loss)\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # MODEL SPECIFICATION\n",
    "    # Create the model specification and return it\n",
    "    # It contains nodes or operations in the graph that will be used for training and evaluation\n",
    "    model_spec = inputs\n",
    "    variable_init_op = tf.group(*[tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "    model_spec['variable_init_op'] = variable_init_op\n",
    "    model_spec[\"predictions\"] = predictions\n",
    "    model_spec['loss'] = loss\n",
    "    model_spec['accuracy'] = accuracy\n",
    "    model_spec['metrics_init_op'] = metrics_init_op\n",
    "    model_spec['metrics'] = metrics\n",
    "    model_spec['update_metrics'] = update_metrics_op\n",
    "    model_spec['summary_op'] = tf.summary.merge_all()\n",
    "\n",
    "    if is_training:\n",
    "        model_spec['train_op'] = train_op\n",
    "\n",
    "    return model_spec"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
